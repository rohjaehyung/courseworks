{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7116ddd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jaehyungroh/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jaehyungroh/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/jaehyungroh/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaehyungroh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jaehyungroh/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') #tokenizer\n",
    "nltk.download('wordnet') #lemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger') # Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5310ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"He is a good guy, he is not bad\"\n",
    "d2 = \"feet wolves cooked boys girls, !<@!\"\n",
    "d3 = \"He is not a good guy, he is bad\"\n",
    "\n",
    "c1 = [d1, d2, d3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0633c832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'is', 'a', 'good', 'guy', ',', 'he', 'is', 'not', 'bad']\n"
     ]
    }
   ],
   "source": [
    "# tokenizer \n",
    "token_d1 = nltk.word_tokenize(d1)\n",
    "print(token_d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a2a906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'is', 'a', 'good', 'guy,', 'he', 'is', 'not', 'bad']\n"
     ]
    }
   ],
   "source": [
    "# tokenizer w/ whitespace\n",
    "tokenizer1 = nltk.tokenize.WhitespaceTokenizer()\n",
    "token_d12 = tokenizer1.tokenize(d1)\n",
    "print(token_d12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320056c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb4d5302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'he': 7, 'is': 8, 'good': 5, 'guy': 6, 'not': 9, 'bad': 0, 'feet': 3, 'wolves': 10, 'cooked': 2, 'boys': 1, 'girls': 4}\n"
     ]
    }
   ],
   "source": [
    "vectorizer1 = CountVectorizer()\n",
    "vectorizer1.fit(c1) # find the basis for vector: bag of words\n",
    "print(vectorizer1.vocabulary_) # number represent index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a91b7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 1 1 2 2 1 0]\n",
      " [0 1 1 1 1 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 1 1 2 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "v1 = vectorizer1.transform(c1)\n",
    "print(v1.toarray()) # vector representation of counts list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5c5359f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feet', 'wolves', 'cooked', 'boys', 'girls', ',', '!', '<', '@', '!']\n"
     ]
    }
   ],
   "source": [
    "# Stemming - the process of transforming a word into its stem(root)\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "token_d2 = nltk.word_tokenize(d2)\n",
    "print(token_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff9ffbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feet', 'wolv', 'cook', 'boy', 'girl']\n"
     ]
    }
   ],
   "source": [
    "#loop to iterate to the root\n",
    "stemmered_token_d2 = [stemmer.stem(token) for token in token_d2 \n",
    "                      if token.isalpha()]\n",
    "print(stemmered_token_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58b804a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foot', 'wolf', 'cooked', 'boy', 'girl']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization - the process to use a pre-defined dictionary to lookup lemmas\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemmatized_token_d2 = []\n",
    "\n",
    "for token in token_d2:\n",
    "    if token.isalpha():\n",
    "        lemmatized_token_d2.append(lemmatizer.lemmatize(token))\n",
    "        # if word, change the word into rootform using lematizer and put into the list\n",
    "\n",
    "print(lemmatized_token_d2)\n",
    "\n",
    "# often used Stemmer & Lemmatization to cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "519cb639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'good', 'guy', ',', 'bad']\n"
     ]
    }
   ],
   "source": [
    "# Stop words - articles, prepositions \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_removed = [token for token in token_d1 \n",
    "                     if not token in stopwords.words('english')]\n",
    "print(stopwords_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fde36132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'he': 3, 'is': 4, 'good': 1, 'guy': 2, 'not': 5, 'bad': 0}\n"
     ]
    }
   ],
   "source": [
    "# low frequency\n",
    "vectorizer2 = CountVectorizer(min_df = 2)\n",
    "vectorizer2.fit(c1)\n",
    "print(vectorizer2.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0d77c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency - Inversed Document Frequency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cfc2b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28867513 0.         0.         0.         0.         0.28867513\n",
      "  0.28867513 0.57735027 0.57735027 0.28867513 0.        ]\n",
      " [0.         0.4472136  0.4472136  0.4472136  0.4472136  0.\n",
      "  0.         0.         0.         0.         0.4472136 ]\n",
      " [0.28867513 0.         0.         0.         0.         0.28867513\n",
      "  0.28867513 0.57735027 0.57735027 0.28867513 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer3 = TfidfVectorizer()\n",
    "vectorizer3.fit(c1)\n",
    "v3 = vectorizer3.transform(c1)\n",
    "print(v3.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2823ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'he': 5, 'is': 7, 'good': 1, 'guy': 3, 'not': 9, 'bad': 0, 'he is': 6, 'good guy': 2, 'guy he': 4, 'is not': 8}\n"
     ]
    }
   ],
   "source": [
    "# ngram\n",
    "vectorizer4 = TfidfVectorizer(ngram_range = (1,2), min_df=2)\n",
    "vectorizer4.fit(c1)\n",
    "print(vectorizer4.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d853c3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('drink', 'VBP'), ('water', 'NN'), ('in', 'IN'), ('parties', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "# Part of Speech (POS)\n",
    "d4 = \"I drink water in parties\"\n",
    "d5 = \"I grab a drink in parties\"\n",
    "\n",
    "c2 = [d4, d5]\n",
    "\n",
    "token_d4 = nltk.word_tokenize(d4)\n",
    "POS_token_d4 = nltk.pos_tag(token_d4)\n",
    "\n",
    "print(POS_token_d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17734f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I_PRP drink_VBP water_NN in_IN parties_NNS', 'I_PRP grab_VBP a_DT drink_NN in_IN parties_NNS']\n",
      "['I drink water in parties', 'I grab a drink in parties']\n"
     ]
    }
   ],
   "source": [
    "# especially for assignment\n",
    "# 1. tokenize\n",
    "# 2. Stemming & Lemmatizing & remove Stopwords & POS tag\n",
    "# 3. Returns with list of tokens\n",
    "# 4. CANNOT be accepted into vectorizer - will cause syntaxerror\n",
    "# 5. Before applying vectorizer, join list of tokens back to text (sentences) for each document\n",
    "\n",
    "POS_c2 = [] # list of re-joined docs after applying POS tag\n",
    "\n",
    "for doc in c2:\n",
    "    token_doc = nltk.word_tokenize(doc)\n",
    "    POS_token_doc = nltk.pos_tag(token_doc)\n",
    "    temp = [] # to store the concated token (token_POS)\n",
    "    for i in POS_token_doc:\n",
    "        temp.append(i[0]+\"_\"+i[1])\n",
    "        \n",
    "    # join the POS tagged taken back to a doc (sentence)\n",
    "    POS_c2.append(\" \".join(temp)) # shorter version\n",
    "    #POS_c2.append(\" \".join(j for j in temp)) - alternative way\n",
    "\n",
    "print(POS_c2)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73433393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
