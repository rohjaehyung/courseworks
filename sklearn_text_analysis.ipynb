{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "\n",
    "This example shows a few text analyis methods: similarity, clustering, topic modeling and sentiment analysis.\n",
    "\n",
    "## Data\n",
    "\n",
    "Data is typically input as a list of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import * # Jaccard\n",
    "from sklearn.metrics.pairwise import * # Cosine\n",
    "\n",
    "d1 = \"He is a good guy, he is not bad\"\n",
    "d2 = \"feet wolves cooked boys girls ,!<@!\"\n",
    "d3 = \"He is not a good guy, he is bad\"\n",
    "d4 = \"I drink water in parties\"\n",
    "d5 = \"I grab a drink in parties\"\n",
    "\n",
    "c3 = [d4, d5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer\n",
    "\n",
    "`fit()` finds BOW (Bag of Words) and generate vocabulary (indexed alphabetically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'drink': 0, 'water': 4, 'in': 2, 'parties': 3, 'grab': 1}\n"
     ]
    }
   ],
   "source": [
    "vectorizer5 = CountVectorizer()\n",
    "vectorizer5.fit(c3)\n",
    "print(vectorizer5.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transform()` converts each word to a vector spanned by the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_c3 = vectorizer5.transform(c3).toarray()\n",
    "v_c3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarity** - cosine_similarity needs input as lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.75]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cosine_similarity([v_c3[0]], [v_c3[1]]))\n",
    "jaccard_score(v_c3[0],v_c3[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Analysis\n",
    "\n",
    "We use K-Means and Agglomerative Clustering for clustering analysis.\n",
    "\n",
    "### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "d6 = \"Seattle weather is bad in winter\"\n",
    "d7 = \"Seattle Seahawks is a great football team\"\n",
    "d8 = \"I love Seahawks\"\n",
    "d9 = \"I learned a lot of Data analytics tools\"\n",
    "d10 = \"I am a data scientist\"\n",
    "c4 = [d1,d2,d3,d4,d5,d6,d7,d8,d9,d10]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer6 = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer6.fit_transform(c4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "\n",
    "`fit()` to train the model which gives cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23007895, 0.        , 0.        , 0.        , 0.51943254,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.23007895, 0.23007895,\n",
       "        0.        , 0.        , 0.38095248, 0.        , 0.        ,\n",
       "        0.        , 0.23007895, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.10272995, 0.1118034 , 0.1118034 , 0.        ,\n",
       "        0.        , 0.1118034 , 0.11857386, 0.1118034 , 0.        ,\n",
       "        0.        , 0.11857386, 0.        , 0.        , 0.        ,\n",
       "        0.19047624, 0.        , 0.        , 0.26272082, 0.21822012,\n",
       "        0.11857386, 0.        , 0.        , 0.13812811, 0.13812811,\n",
       "        0.1118034 ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.54362395, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.31974443, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54362395, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.31974443, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.52610083, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.6013393 ,\n",
       "        0.        , 0.        , 0.6013393 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 4 # number of clusters\n",
    "model1 = KMeans(n_clusters = k)\n",
    "model1.fit(X)\n",
    "model1.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transform()` assigns cluster membership as given by labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 3, 2, 2, 0, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit_transform(X)\n",
    "model1clusters = model1.labels_.tolist()\n",
    "model1clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characteristics of each cluster\n",
    "\n",
    "We want to find 5 most significant words for each cluster. \n",
    "\n",
    "1. for each cluster, we get the indices of sorted array in descending order. `argsort()` sorts the array in ascending order and return the indices of sorted array. The index slicing  `[:,::-1]`reads the sorted array from the end and effectively re-arrange it in descending order.\n",
    "2. `get_feature_names_out()` returns the words corresponding to indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "seahawks\n",
      "seattle\n",
      "love\n",
      "weather\n",
      "Cluster 1:\n",
      "data\n",
      "scientist\n",
      "analytics\n",
      "tools\n",
      "Cluster 2:\n",
      "drink\n",
      "parties\n",
      "water\n",
      "grab\n",
      "Cluster 3:\n",
      "guy\n",
      "good\n",
      "bad\n",
      "winter\n"
     ]
    }
   ],
   "source": [
    "order_centroids = model1.cluster_centers_.argsort()[:,::-1] \n",
    "terms = vectorizer6.get_feature_names_out()\n",
    "\n",
    "for c in range(4):\n",
    "    print('Cluster %d:' % c)\n",
    "    for ind in order_centroids[c, :4]:\n",
    "        print(terms[ind])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering\n",
    "\n",
    "1. **n_clusters** - number of clusters pre-selected\n",
    "2. **affinity** - distance measure between documents, others are: manhattan, cosince\n",
    "3. **linkage** - distance measure between clusters, others are: single, complete, average  \n",
    "\n",
    "`fit_predict()` fits the hierarchical clustering from features or distance matrix, and returns cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 3 2 2 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "model2 = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\n",
    "model2.fit_predict(X.toarray())\n",
    "print(model2.labels_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA and Topic Modeling\n",
    "\n",
    "### Data and document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer7 = CountVectorizer(stop_words='english')\n",
    "X2 = vectorizer7.fit_transform(c4)\n",
    "terms = vectorizer7.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "* **n_components** - number of topics\n",
    "* The model **lda**'s attribute **components_** stores topic word distribution. The array **components_[i, j]** can be viewed as pseudocount that represents the number of times **word j** was assigned to **topic i**.\n",
    "\n",
    "To display the representative words under each topic, for each topic:\n",
    "1. sort the indices of words in the descending order of pseucount\n",
    "2. take the top 4 \n",
    "2. retrieve the corresponding words and join them with space \" \" inbetween them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "wolves boys cooked feet\n",
      "Topic 1:\n",
      "bad guy good parties\n",
      "Topic 2:\n",
      "seahawks seattle great team\n",
      "Topic 3:\n",
      "scientist grab water seattle\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=4).fit(X2)\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx))\n",
    "    print(\" \".join([terms[i] for i in topic.argsort()[:-4-1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability of a document containing a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06257622, 0.81221881, 0.06256438, 0.06264059]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(X2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA using gensim\n",
    "\n",
    "#### Prepare the data\n",
    "\n",
    "list of lists of words from list of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['good', 'guy', 'bad'], ['foot', 'wolf', 'cook', 'boy', 'girl'], ['good', 'guy', 'bad'], ['drink', 'water', 'parti'], ['grab', 'drink', 'parti'], ['seattl', 'weather', 'bad', 'winter'], ['seattl', 'seahawk', 'great', 'footbal', 'team'], ['love', 'seahawk'], ['learn', 'lot', 'data', 'analyt', 'tool'], ['data', 'scientist']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "processed_c4 = []\n",
    "for doc in c4:\n",
    "    tokens = nltk.word_tokenize(doc.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    tokens = [token for token in tokens if not token in stopwords.words('english')]\n",
    "    processed_c4.append(tokens)\n",
    "    \n",
    "print(processed_c4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gensim \n",
    "\n",
    "In terminal, **pip install gensim**\n",
    "\n",
    "1. create the **dictionary** which is a list of words from all documents\n",
    "2. convert document into the bag-of-words format = list of (word_id, word_count) 2-tuples\n",
    "3. train the model with number of topics **num_topics** and mapping from word IDs to words **id2word** \n",
    "4. use **`print_topics(num_topics, num_words)`** to display **num_topics** randomly selected topics with a string of **num_words** words ordered by their significance. Both are optional with the default `num_topics=20, num_words=10`. `num_topics = -1` means to show all topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "Topic: 0 \n",
      " Words: 0.164*\"seahawk\" + 0.092*\"seattl\" + 0.092*\"great\" + 0.091*\"footbal\"\n",
      "Topic: 1 \n",
      " Words: 0.092*\"girl\" + 0.091*\"boy\" + 0.091*\"foot\" + 0.091*\"cook\"\n",
      "Topic: 2 \n",
      " Words: 0.117*\"parti\" + 0.117*\"drink\" + 0.109*\"bad\" + 0.066*\"good\"\n",
      "Topic: 3 \n",
      " Words: 0.094*\"bad\" + 0.082*\"lot\" + 0.082*\"guy\" + 0.082*\"analyt\"\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(processed_c4)\n",
    "bow_c4 = [dictionary.doc2bow(doc) for doc in processed_c4]\n",
    "print(bow_c4[1])\n",
    "\n",
    "lda_model = gensim.models.LdaModel(bow_c4, num_topics=4, id2word=dictionary)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1, 4):\n",
    "    print('Topic: {} \\n Words: {}'.format(idx, topic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "### Install vader_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ytan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"They are smart, cute, and funny.\",  # positive sentence example\n",
    "    \"They are smart, cute, and funny!\", # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "    \"They are very smart, cute, and funny.\",# booster words handled correctly (sentiment intensity adjusted)\n",
    "    \"They are VERY SMART, cute, and FUNNY.\",  # emphasis for ALLCAPS handled\n",
    "    \"They are VERY SMART, cute, and FUNNY!!!\",  # combination of signals - VADER appropriately adjusts intensity\n",
    "    \"They are VERY SMART, really handsome, and INCREDIBLY FUNNY!!!\",  # booster words & punctuation make this close to ceiling for score\n",
    "    \"The book was good.\",  # positive sentence\n",
    "    \"The book was kind of good.\",  # qualified positive sentence is handled correctly (intensity adjusted)\n",
    "    \"The plot was good, but the characters are uncompelling and the dialog is not great.\",  # mixed negation sentence\n",
    "    \"A really bad, horrible book.\",  # negative sentence with booster words\n",
    "    \"At least it isn't a horrible book.\",  # negated negative sentence with contraction\n",
    "    \":) and :D\",  # emoticons handled\n",
    "    \"\",  # an empty string is correctly handled\n",
    "    \"Today sux\",  # negative slang handled\n",
    "    \"Today sux!\",  # negative slang with punctuation emphasis handled\n",
    "    \"Today SUX!\",  # negative slang with capitalization emphasis\n",
    "    \"Today kinda sux! But I'll get by, lol\"  # mixed sentiment example with slang and contrastive conjunction \"but\"\n",
    "     ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis with existing classifier\n",
    "\n",
    "1. use `SentimentIntensityAnalyzer()`\n",
    "2. get polarity scores: compound, neg, neu, pos. The output is a dictionary data type which is joined by a dictionary of sentence using `update()`. \n",
    "3. convert list of dictionary to `DataFrame` and show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They are smart, cute, and funny.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.8225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They are smart, cute, and funny!</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.8356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They are very smart, cute, and funny.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.8470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They are VERY SMART, cute, and FUNNY.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.9196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They are VERY SMART, cute, and FUNNY!!!</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.9318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>They are VERY SMART, really handsome, and INCR...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.9469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The book was good.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The book was kind of good.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.3832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The plot was good, but the characters are unco...</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.7042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A really bad, horrible book.</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.8211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>At least it isn't a horrible book.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.4310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>:) and :D</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.7925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Today sux</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Today sux!</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Today SUX!</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Today kinda sux! But I'll get by, lol</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.5249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence    neg    neu    pos  \\\n",
       "0                    They are smart, cute, and funny.  0.000  0.259  0.741   \n",
       "1                    They are smart, cute, and funny!  0.000  0.252  0.748   \n",
       "2               They are very smart, cute, and funny.  0.000  0.304  0.696   \n",
       "3               They are VERY SMART, cute, and FUNNY.  0.000  0.249  0.751   \n",
       "4             They are VERY SMART, cute, and FUNNY!!!  0.000  0.236  0.764   \n",
       "5   They are VERY SMART, really handsome, and INCR...  0.000  0.294  0.706   \n",
       "6                                  The book was good.  0.000  0.508  0.492   \n",
       "7                          The book was kind of good.  0.000  0.657  0.343   \n",
       "8   The plot was good, but the characters are unco...  0.327  0.579  0.094   \n",
       "9                        A really bad, horrible book.  0.791  0.209  0.000   \n",
       "10                 At least it isn't a horrible book.  0.000  0.637  0.363   \n",
       "11                                          :) and :D  0.000  0.124  0.876   \n",
       "12                                                     0.000  0.000  0.000   \n",
       "13                                          Today sux  0.714  0.286  0.000   \n",
       "14                                         Today sux!  0.736  0.264  0.000   \n",
       "15                                         Today SUX!  0.779  0.221  0.000   \n",
       "16              Today kinda sux! But I'll get by, lol  0.138  0.517  0.344   \n",
       "\n",
       "    compound  \n",
       "0     0.8225  \n",
       "1     0.8356  \n",
       "2     0.8470  \n",
       "3     0.9196  \n",
       "4     0.9318  \n",
       "5     0.9469  \n",
       "6     0.4404  \n",
       "7     0.3832  \n",
       "8    -0.7042  \n",
       "9    -0.8211  \n",
       "10    0.4310  \n",
       "11    0.7925  \n",
       "12    0.0000  \n",
       "13   -0.3612  \n",
       "14   -0.4199  \n",
       "15   -0.5461  \n",
       "16    0.5249  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "scores = []\n",
    "for sentence in sentences:\n",
    "    score = {'sentence': sentence}\n",
    "    score.update(sid.polarity_scores(sentence))   \n",
    "    scores.append(score)\n",
    "\n",
    "df_scores = pd.DataFrame(scores)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
