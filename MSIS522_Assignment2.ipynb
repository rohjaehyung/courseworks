{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "MSIS522-Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bSsc9tL7hs1k"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bYyOjdzhs1T",
        "colab_type": "text"
      },
      "source": [
        "# Decision Tree & Ensemble Learning\n",
        "\n",
        "Classification And Regression Trees (CART for short) is a term introduced by [Leo Breiman](https://en.wikipedia.org/wiki/Leo_Breiman) to refer to Decision Tree algorithms that can be used for classification or regression predictive modeling problems.\n",
        "\n",
        "In this lab assignment, you will implement various ways to calculate impurity which is used to split data in constructing the decision trees and apply the Decision Tree and ensemble learning algorithms to solve two real-world problems: a classification one and a regression one. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsJh8RbLhs1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import packages\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from matplotlib.legend_handler import HandlerLine2D\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# make this notebook's output stable across runs\n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wdJsuTqhs1a",
        "colab_type": "text"
      },
      "source": [
        "## Gini impurity and Entropy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzfYaZ-hhs1b",
        "colab_type": "text"
      },
      "source": [
        "#### Gini impurity\n",
        "\n",
        "The CART algorithm recursively splits the training set into two subsets using a single feature k and a threshold $t_k$. The best feature and threshold are chosen to produce the purest subsets weighted by their size. **Gini impurity** measures the impurity of the data points in a set and is used to evaluate how good a split is when the CART algorithm searches for the best pair of feature and the threshold.\n",
        "\n",
        "To compute Gini impurity for a set of items with J classes, suppose $i \\in \\{1, 2, \\dots, J\\}$ and let $p_i$ be the fraction of items labeled with class i in the set.\n",
        "\\begin{align}\n",
        "I(p) = 1 - \\sum_{i=1}^J p_i^2\n",
        "\\end{align}\n",
        "\n",
        "The following function calculates the gini impurity for a given set of data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNexBvnehs1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gini_impurity(x):\n",
        "    \"\"\"\n",
        "    This function calculate the Gini impurity for a given set of data points.\n",
        "\n",
        "    Args:\n",
        "    x: a numpy ndarray\n",
        "    \"\"\"\n",
        "    unique, counts = np.unique(x, return_counts=True)\n",
        "    probabilities = counts / sum(counts)\n",
        "    gini = 1 - sum([p*p for p in probabilities])\n",
        "\n",
        "    return gini"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmZaWVMxhs1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.testing.assert_equal(0, gini_impurity(np.array([1, 1, 1])))\n",
        "np.testing.assert_equal(0.5, gini_impurity(np.array([1, 0, 1, 0])))\n",
        "np.testing.assert_equal(3/4, gini_impurity(np.array(['a', 'b', 'c', 'd'])))\n",
        "np.testing.assert_almost_equal(2.0/3, gini_impurity(np.array([1, 2, 3, 1, 2, 3])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSsc9tL7hs1k",
        "colab_type": "text"
      },
      "source": [
        "#### Entropy\n",
        "\n",
        "Another popular measure of impurity is called **entropy**, which measures the average information content of a message. Entropy is zero when all messages are identical. When it applied to CART, a set's entropy is zero when it contains instances of only one class. Entropy is calculated as follows:\n",
        "\\begin{align}\n",
        "I(p) = - \\sum_{i=1}^J p_i log_2{p_i}\n",
        "\\end{align}\n",
        "\n",
        "<span style=\"color:orange\">**Question 1: In this exercise, you will implement the entropy function.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr8caP2Xhs1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entropy(x):\n",
        "    \"\"\"\n",
        "    TODO: This function calculate the entropy of an array.\n",
        "\n",
        "    Args:\n",
        "    x: a numpy ndarray\n",
        "    \"\"\"\n",
        "    e = 0 # TODO\n",
        "\n",
        "    return e"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpOdGMWDhs1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.testing.assert_equal(0, entropy(np.array([1, 1, 1])))\n",
        "np.testing.assert_equal(1.0, entropy(np.array([1, 0, 1, 0])))\n",
        "np.testing.assert_equal(2.0, entropy(np.array(['a', 'b', 'c', 'd'])))\n",
        "np.testing.assert_almost_equal(1.58496, entropy(np.array([1, 2, 3, 1, 2, 3])), 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qX1XT12hs1t",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZGblkNkhs2g",
        "colab_type": "text"
      },
      "source": [
        "## Iris dataset\n",
        "\n",
        "The Iris data set contains the morphologic variation of Iris flowers of three related species (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each observation (see image below):\n",
        "- Sepal.Length: sepal length in centimeters.\n",
        "- Sepal.Width: sepal width in centimeters.\n",
        "- Petal.Length: petal length in centimeters.\n",
        "- Petal.Width: petal width in centimeters.\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg/180px-Kosaciec_szczecinkowaty_Iris_setosa.jpg\" style=\"width:250px\"></td>\n",
        "    <td><img src=\"https://www.math.umd.edu/~petersd/666/html/iris_with_labels.jpg\" width=\"250px\"></td>\n",
        "    <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/295px-Iris_virginica.jpg\" width=\"250px\"></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Iris setosa</td>\n",
        "    <td>Iris versicolor</td>\n",
        "    <td>Iris virginica</td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BfuNcWVhs2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the iris train and test data from CSV files\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/iris_train.csv')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/iris_test.csv')\n",
        "\n",
        "train_x = train.iloc[:,0:4]\n",
        "train_y = train.iloc[:,4]\n",
        "\n",
        "test_x = test.iloc[:,0:4]\n",
        "test_y = test.iloc[:,4]\n",
        "\n",
        "# print the number of instances in each class\n",
        "print(train_y.value_counts().sort_index())\n",
        "print(test_y.value_counts().sort_index())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpe8jv7Qhs2f",
        "colab_type": "text"
      },
      "source": [
        "### Decision Tree Classifier\n",
        "\n",
        "<span style=\"color:orange\">**In this exercise, we will apply the Decision Tree classifier to classify the Iris flower data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JwQoOdPhs2j",
        "colab_type": "text"
      },
      "source": [
        "#### Train and visualize a simple Decision Tree\n",
        "\n",
        "<span style=\"color:orange\">**Question 2: create a decision tree with max_depth of 2.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kQzTTFGlhs2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: read the scikit-learn doc on DecisionTreeClassifier and train a Decision Tree with max depth of 2\n",
        "dtc = # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn25MVpchs2n",
        "colab_type": "text"
      },
      "source": [
        "Now let's visualize the decision tree we just trained on the iris dataset and see how it makes predictions. Note that if the following code does not work for you because the graphviz is missing, do not worry about it and you should still be able to move on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Y80Nilx1hs2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.externals.six import StringIO  \n",
        "from IPython.display import Image  \n",
        "from sklearn.tree import export_graphviz\n",
        "import pydotplus\n",
        "\n",
        "dot_data = StringIO()\n",
        "feature_names = train_x.columns\n",
        "class_names = train_y.unique()\n",
        "class_names.sort()\n",
        "export_graphviz(dtc, out_file=dot_data, feature_names=feature_names, class_names=class_names, filled=True, rounded=True)\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "Image(graph.create_png())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgqeLw6Lhs2t",
        "colab_type": "text"
      },
      "source": [
        "Decision trees are easy to inteprete and is often referred to as *whitebox* machine learning algorithm. Let's see how this decision tree represented above makes predictions. Suppose you find an iris flower and want to classify it into setosa, versicolor or virginica. You start at the root node (the very top node in the tree). In this node, we check if the flower's patel length is smaller than or equal to 2.35 cm. If it is, we move to the left child and predict setosa to be its class. Otherwise, we move to the right child node. Then similarly we check if the petal length is smaller than or equal to 4.95 cm. If it is, we move to its left child node and predict versicolor to be its class. Otherwise, we move to its right child and predict virginica to be its class. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W94pWMvIhs2t",
        "colab_type": "text"
      },
      "source": [
        "#### Prediction with Decision tree\n",
        "\n",
        "With this simple decision tree above, we can apply it to make predictions on the test dataset and evaluate its performance.\n",
        "\n",
        "<span style=\"color:orange\">**Question 3: make prediction using the trained decision tree model on the test data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugRPudSThs2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: use the trained decision tree model to make predictions on the test data and evaluate the model performance.\n",
        "test_z = # TODO\n",
        "\n",
        "print(\"model accuracy: {}\".format(accuracy_score(test_y, test_z)))\n",
        "print(\"model confusion matrix:\\n {}\".format(confusion_matrix(test_y, test_z, labels=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMzcZjQQhs2x",
        "colab_type": "text"
      },
      "source": [
        "#### Hyper-parameters\n",
        "\n",
        "Hyper-parameter controls the complexity of the decision tree model. For example, the deeper the tree is, the more complex patterns the model will be able to capture. In this exercise, we train the decision trees with increasing number of maximum depth and plot its performance. We should see the accuracy of the training data increase as the tree grows deeper, but the accuracy on the test data might not as the model will eventually start to overfit and does not generalize well on the unseen test data.\n",
        "\n",
        "<span style=\"color:orange\">**Question 4: for each value of max_depth, we train a decision tree model and evaluate its accuracy on both train and test data, and plot both accuracies in the figure.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw23r_MXhs2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: train the decision tree model with various max_depth, make predictions and evaluate on both train and test data.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iaLr1ufhs20",
        "colab_type": "text"
      },
      "source": [
        "#### Fine-tune the decision tree classifier\n",
        "\n",
        "Decision tree is a very powerful model with very few assumptions about the incoming training data (unlike linear models, which assume the data linear), however, it is more likely to overfit the data and won't generalize well to unseen data. To void overfitting, we need to restrict the decision tree's freedom during training via regularization (e.g. max_depth, min_sample_split, max_leaf_nodes and etc.).\n",
        "\n",
        "To fine-tune the model and combat overfitting, use grid search with cross-validation (with the help of the GridSearchCV class) to find the best hyper-parameter settings for the DecisionTreeClassifier. In particular, we would like to fine-tune the following hyper-parameters:\n",
        "- **criteria**: this defines how we measure the quality of a split. we can choose either \"gini\" for the Gini impurity or \"entropy\" for the information gain.\n",
        "- **max_depth**: the maximum depth of the tree. This indicates how deep the tree can be. The deeper the tree, the more splits it has and it captures more information about the data. But meanwhile, deeper trees are more likely to overfit the data. For this practice, we will choose from {1, 2, 3} given there are only 4 features in the iris dataset.\n",
        "- **min_samples_split**: This value represents the minimum number of samples required to split an internal node. The smaller this value is, the deeper the tree will grow, thus more likely to overfit. On the other hand, if the value is really large (the size of the training data in the extreme case), the tree will be very shallow and could suffer from underfit. In this practice, we choose from {0.01, 0.05, 0.1, 0.2}.\n",
        "\n",
        "<span style=\"color:orange\">**Question 5: Use grid search with 3-fold cross-validation to fine-tune the decision tree model and output the best hyper-parameters.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fuI5TWkhs21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: fine-tune the model, use grid search with 3-fold cross-validation.\n",
        "parameters = # TODO\n",
        "\n",
        "dt = # TODO\n",
        "grid = # TODO\n",
        "\n",
        "# summarize the results of the grid search\n",
        "print(\"The best score is {}\".format(grid.best_score_))\n",
        "print(\"The best hyper parameter setting is {}\".format(grid.best_params_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXeHA4TThs24",
        "colab_type": "text"
      },
      "source": [
        "#### Prediction and Evaluation\n",
        "\n",
        "Now we have a fine-tuned decision tree classifier based on the training data, let's apply this model to make predictions on the test data and evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzGEUWFPhs24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid.predict(test_x)\n",
        "\n",
        "print(\"model accuracy: {}\".format(accuracy_score(test_y, test_z)))\n",
        "print(\"model confusion matrix:\\n {}\".format(confusion_matrix(test_y, test_z, labels=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLrLCXpbIHI2",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest\n",
        "\n",
        "**Question 6: Apply Random Forest together with Gridsearch to the Iris dataset and evaluate its accuracy.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTzDdKnoIDM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TODO "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKgNC7kzIWNC",
        "colab_type": "text"
      },
      "source": [
        "### Adaboost\n",
        "\n",
        "**Question 7: Apply Adaboost together with Gridsearch to the Iris dataset and evaluate its accuracy.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDcP-xy_IXFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TODO "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpZFN60NIWdK",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Boosting\n",
        "\n",
        "**Question 8: Apply Boosting together with Gridsearch to the Iris dataset and evaluate its accuracy.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmRcvEkSIXrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TODO "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTlPSt2Uhs26",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oyoZxHnxHknE"
      },
      "source": [
        "**BONUS POINT: we will apply the supervised learning models we learnt so far to predict the California housing prices.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3odrweXdhs28",
        "colab_type": "text"
      },
      "source": [
        "## California Housing Dataset\n",
        "\n",
        "The California Housing dataset appeared in a 1997 paper titled Sparse Spatial Autoregressions by Pace, R. Kelley and Ronald Barry, published in the Statistics and Probability Letters journal. They built it using the 1990 California census data. It contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zXK2RFshs29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load train and test data from CSV files.\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/housing_train.csv')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/housing_test.csv')\n",
        "\n",
        "train_x = train.iloc[:,0:8]\n",
        "train_y = train.iloc[:,8]\n",
        "\n",
        "test_x = test.iloc[:,0:8]\n",
        "test_y = test.iloc[:,8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-Xv_wdQJob8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-zitGTxhs3E",
        "colab_type": "text"
      },
      "source": [
        "### End of Assignment 2\n",
        "---\n"
      ]
    }
  ]
}